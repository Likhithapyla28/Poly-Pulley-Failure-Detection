{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 810 entries, 0 to 809\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_path  810 non-null    object\n",
      " 1   label      810 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "new_size = (64, 64)\n",
    "resized_images = []\n",
    "images=[]\n",
    "for img in images:\n",
    "    try:\n",
    "        # Ensure the array is of dtype uint8 and has 3 dimensions (height, width, channels)\n",
    "        img_array = np.asarray(img, dtype=np.uint8)\n",
    "        if img_array.ndim == 2:\n",
    "            # Add a channel dimension for grayscale images\n",
    "            img_array = np.expand_dims(img_array, axis=-1)\n",
    "        elif img_array.ndim != 3 or img_array.shape[-1] != 3:\n",
    "            raise ValueError(\"Invalid image array shape\")\n",
    "\n",
    "        # Resize the image\n",
    "        resized_img_array = np.array(Image.fromarray(img_array).resize(new_size))\n",
    "        resized_images.append(resized_img_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "\n",
    "\n",
    "\n",
    "# Assuming target size for InceptionV3 is (299, 299)\n",
    "target_size = (299, 299)\n",
    "\n",
    "def load_and_preprocess_image(file_path, target_size=target_size):\n",
    "    image = tf.keras.preprocessing.image.load_img(file_path, target_size=target_size)\n",
    "    image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image_array = tf.keras.applications.inception_v3.preprocess_input(image_array)\n",
    "    return image_array\n",
    "\n",
    "def create_model(target_size):\n",
    "    base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(target_size[0], target_size[1], 3))\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_generator, val_generator, epochs=10):\n",
    "    lr_schedule = LearningRateScheduler(lambda epoch: 1e-3 if epoch < 5 else (1e-4 if 5 <= epoch < 10 else 1e-5))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[lr_schedule, early_stopping])\n",
    "\n",
    "def evaluate_model(model, test_generator):\n",
    "    results = model.evaluate(test_generator)\n",
    "    print(\"Test Loss:\", results[0])\n",
    "    print(\"Test Accuracy:\", results[1])\n",
    "\n",
    "# Paths for clean and dirt images\n",
    "clean_folder = \"C:/Users/likhitha/Downloads/data/CleanSyntheticData\"\n",
    "dirt_folder = \"C:/Users/likhitha/Downloads/data/DirtSyntheticData\"\n",
    "# Get a list of file paths for clean and dirt images\n",
    "clean_files = [os.path.join(clean_folder, file) for file in os.listdir(clean_folder)]\n",
    "dirt_files = [os.path.join(dirt_folder, file) for file in os.listdir(dirt_folder)]\n",
    "\n",
    "# Create a DataFrame with file paths and labels (0 for clean, 1 for dirt)\n",
    "data = {'file_path': clean_files + dirt_files, 'label': [0] * len(clean_files) + [1] * len(dirt_files)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, you can shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame information\n",
    "print(\"DataFrame Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Load and preprocess images\n",
    "df['image'] = df['file_path'].apply(load_and_preprocess_image)\n",
    "\n",
    "# Standardize pixel values\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Resize and load images into NumPy array\n",
    "images = np.array([np.array(image) for image in df['image']])\n",
    "\n",
    "# Standardize pixel values directly\n",
    "images_standardized = scaler.fit_transform(images.reshape(images.shape[0], -1))\n",
    "\n",
    "# Ensure that the standardized array has a consistent shape\n",
    "images_standardized = np.vstack(images_standardized).reshape(images.shape)\n",
    "\n",
    "print(len(df['file_path']))\n",
    "print(len(df['label']))\n",
    "print(len(images_standardized))\n",
    "\n",
    "# Output the DataFrame with standardized pixel values\n",
    "df_final = pd.DataFrame({'file_path': df['file_path'], 'label': df['label'].astype(str), 'image': images_standardized.tolist()})\n",
    "print(\"\\nDataFrame with Standardized Pixel Values:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Get a list of file paths for clean and dirt images\n",
    "clean_files = [os.path.join(clean_folder, file) for file in df_final['file_path']]\n",
    "dirt_files = [os.path.join(dirt_folder, file) for file in df_final['file_path']]\n",
    "\n",
    "# Create a DataFrame with file paths and labels\n",
    "# Pad all lists in the 'image' column with zeros\n",
    "max_length = max(len(image) for image in df_final['image'])\n",
    "df_final['image'] = df_final['image'].apply(lambda x: x + [0] * (max_length - len(x)))\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final['file_path'], df_final['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an InceptionV3 base model\n",
    "model = create_model(target_size)\n",
    "\n",
    "# Create data generators with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "# No data augmentation for validation/testing\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 32\n",
    "# Define the validation generator\n",
    "val_generator = test_datagen.flow_from_dataframe(dataframe=df_final, directory=None, x_col='file_path', y_col='label', target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe=df_final, directory=None, x_col='file_path', y_col='label', target_size=target_size, batch_size=batch_size, class_mode='binary', shuffle=True)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_generator = test_datagen.flow_from_dataframe(dataframe=df_final,directory=None,x_col='file_path',y_col='label',target_size=target_size,batch_size=batch_size,class_mode='binary',shuffle=False)\n",
    "# Train and evaluate the model\n",
    "train_model(model, train_generator, val_generator, epochs=10)\n",
    "evaluate_model(model, test_generator)\n",
    "\n",
    "# Load the trained model from the HDF5 file\n",
    "model = tf.keras.models.load_model('model4.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f83a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model including optimizer state\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('my_model.h5')\n",
    "--\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model('model4.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Load the trained model from the HDF5 file\n",
    "model = tf.keras.models.load_model('model4.h5')\n",
    "def load_and_preprocess_new_image(file_path, target_size=target_size):\n",
    "    img = tf.keras.preprocessing.image.load_img(file_path, target_size=target_size)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
    "    return img_array\n",
    "new_images_folder = \"C:/Users/likhitha/OneDrive/Desktop/WEKA User defined datasets/SIH/Test\"\n",
    "new_image_files = [os.path.join(new_images_folder, file) for file in os.listdir(new_images_folder)]\n",
    "new_image_predictions = []\n",
    "\n",
    "for file_path in new_image_files:\n",
    "    # Load and preprocess the new image\n",
    "    new_image = load_and_preprocess_new_image(file_path)\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    prediction_result = model.predict(np.expand_dims(new_image, axis=0))\n",
    "\n",
    "    # Interpret the prediction result\n",
    "    if prediction_result[0][0] > 0.5:\n",
    "        prediction_label = \"Dirt Buildup\"\n",
    "    else:\n",
    "        prediction_label = \"Clean\"\n",
    "\n",
    "    new_image_predictions.append({\n",
    "        'file_path': file_path,\n",
    "        'prediction_label': prediction_label,\n",
    "        'prediction_prob': prediction_result[0][0]\n",
    "    })\n",
    "\n",
    "# Print predictions after processing all images\n",
    "for prediction in new_image_predictions:\n",
    "    print(f\"File: {prediction['file_path']}\")\n",
    "    print(f\"Prediction: {prediction['prediction_label']}\")\n",
    "    print(f\"Prediction Probability: {prediction['prediction_prob']:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
